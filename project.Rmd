---
title: "compromise words project"
author: "hoskisson"
date: "2022-12-04"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Ran `auth_setup_default()` and used browser to give rtweet authentication access.
Then whenever I want to use that same authentication, I run the line `auth_as("default")` in another R file.

```{r}
library(rtweet)
library(quanteda)
library(topicmodels)
library(tidyverse)
```

```{r}
auth_as("default")
```

```{r}
rstats <- search_tweets("#rstats", n = 2, include_rts = FALSE)
colnames(rstats)
rstats[1:5, c("created_at", "text", "id_str")]
```

```{r}
# another function could try:
# lookup_tweets
# collins_timeline <- get_timeline(user="SenatorCollins")

# corpus_collins <- corpus(collins_timeline, 
#                                     docid_field = "id",
#                                     text_field = "full_text")

# id from sen kevin cramer tweet march 19: 1505248314232651776
# tweet url: https://twitter.com/SenKevinCramer/status/1505248314232651776
# max id from kevin cramer tweet march 21: 1505904432088862732
# url: https://twitter.com/SenKevinCramer/status/1505904432088862732

tweets_from_list <- lists_statuses(
  list_id = 63915645,
  n = 100,
  max_id = tailed_tweets
)

tailed_tweets <- tweets_from_list %>%
  select(c(created_at, id, id_str, full_text)) %>%
  tail()
tailed_tweets
```


```{r}
# go backward on a senator's timeline to retrive a Tweet in the right timeframe
# the Tweet ids are used later for retrieving Tweets from the US Senator list

tweets_from_timeline <- get_timeline(
  user="PattyMurray",
  n = 20,
  max_id = tailed_tweets
)

tailed_tweets <- tweets_from_timeline %>%
  select(c(created_at, id, id_str, full_text)) %>%
  tail()

tailed_tweets

# View(tweets_from_timeline)
```

Get senator screen names
```{r}
members <- lists_members(list_id = 63915645 )
senator_handles_list <- members$screen_name
```

Respect for marriage act
```{r}
tweets_from_timeline <- get_timeline(
  user=senator_handles_list,
  max_id = c("1598011580016304128"),
  since_id = c("1592524893316579329")
)
# Take only some fields. Get rid of fields that are lists because they cause an error
tweets_from_timeline_processed<-select(tweets_from_timeline, created_at, id, id_str, full_text, text)
write.csv(tweets_from_timeline_processed, "respect_for_marriage.csv" ,row.names = FALSE)
```




```{r}
# retrieve tweets for Respect for Marriage Act 2022

# first get data frame for max_id
# max_tib <- lookup_tweets(statuses = c("1598011580016304128"))
# since_tib <- lookup_tweets(statuses = c("1592524893316579329"))

start_date <- date("2022-11-15")
end_date <- date("2022-11-29")

statuses_first_batch <- lists_statuses(
    n = 50,
    list_id = 63915645
)
last_status = tail(statuses_first_batch, n = 1)
earliest_batch_date <- last_status$created_at[1]
i <- 0

# empty tibble to store statuses
senate_list_statuses_a <- lists_statuses(n=1, list_id = 63915645)
senate_list_statuses_a <- senate_list_statuses_a[-1,]

statuses1 <- lists_statuses(
    n = 1,
    list_id = 63915645,
    max_id = c("1602402701169422336")
  )

last_status1 = tail(statuses1, n=1)
earliest_batch_date1 <- last_status1$created_at[1]

while(earliest_batch_date > start_date) {
  statuses <- lists_statuses(
    n = 50,
    list_id = 63915645,
    max_id = last_status
  )
  
  last_status = tail(statuses, n = 1)
  earliest_batch_date <- last_status$created_at[1]
  
  print(length(statuses$created_at))
  print(earliest_batch_date)
  
  # append rows
  if(earliest_batch_date < end_date) {
    senate_list_statuses_a <- bind_rows(senate_list_statuses_a, statuses)
  }
  
  
  print(paste("iteration: ", i))
  print(paste("earliest_batch_date: ", earliest_batch_date))
  
  i <- i + 1
}
# # respect_for_marriage_tweets <- lists_statuses(
# tmp_statuses2 <- lists_statuses(
#   n = 2,
#   list_id = 63915645,
#   # max_id = c("1592524893316579329")
#   since_id = c("1592524893316579329")
# )
# tmp_statuses <- select(tmp_statuses, created_at, id, id_str, full_text, text)
# 
# rbind(tmp_statuses, tmp_statuses2)

# write.csv(respect_for_marriage_tweets, "respect_for_marriage.csv" ,row.names = FALSE)
# View(respect_for_marriage_tweets)
```



```{r}
corpus_collins_proc <- tokens(corpus_collins, 
                           remove_punct = TRUE, # remove punctuation
                           remove_numbers = TRUE, # remove numbers
                           remove_symbols = TRUE) %>% # remove symbols (for social media data, could remove everything except letters)
                        tokens_tolower() # remove capitalization
```


```{r}
lemmaData <- read.csv2("baseform_en.tsv", # downloaded from https://github.com/tm4ss/tm4ss.github.io/tree/master/resources
                       sep=",", 
                       header=FALSE, 
                       encoding = "UTF-8", 
                       stringsAsFactors = F)
```

```{r}
corpus_collins_proc <-  tokens_replace(corpus_collins_proc, # "Substitute token types based on vectorized one-to-one matching"
                                    lemmaData$V1, 
                                    lemmaData$V2,
                                    valuetype = "fixed") 

```

```{r}
corpus_collins_proc <- corpus_collins_proc %>%
                             tokens_remove(stopwords("english")) %>%
                             tokens_ngrams(1) 
```

```{r}
#  Create dtm
DTM <- dfm(corpus_collins_proc)

# Minimum
minimumFrequency <- 10
DTM <- dfm_trim(DTM, 
                min_docfreq = minimumFrequency,
                max_docfreq = 99999999)

# keep only letters... brute force
DTM  <- dfm_select(DTM, 
                   pattern = "[a-z]", 
                   valuetype = "regex", 
                   selection = 'keep')
colnames(DTM) <- stringi::stri_replace_all_regex(colnames(DTM), 
                                                 "[^_a-z]","")

DTM <- dfm_compress(DTM, "features")

# We have several rows which do not have any content left. Drop them.

sel_idx <- rowSums(DTM) > 0
DTM <- DTM[sel_idx, ]
# textdata <- textdata[sel_idx, ]
```



```{r}
K <- 5
# Set seed to make results reproducible
set.seed(9161)
topicModel <- LDA(DTM, 
                  K, 
                  method="Gibbs", 
                  control=list(iter = 500, 
                               verbose = 25))
```

```{r}
tmResult <- posterior(topicModel)


# Topics are distributions over the entire vocabulary

beta <- tmResult$terms
glimpse(beta)


# Each doc has a distribution over k topics

theta <- tmResult$topics
glimpse(theta)               

terms(topicModel, 10)

# Top terms per topic. Use top 5 to interpret topics
top5termsPerTopic <- terms(topicModel, 
                           3)
# For the next steps, we want to give the topics more descriptive names than just numbers. Therefore, we simply concatenate the five most likely terms of each topic to a string that represents a pseudo-name for each topic.
topicNames <- apply(top5termsPerTopic, 
                    2, 
                    paste, 
                    collapse=" ")
```

What are the most common themes?
```{r}

topicProportions <- colSums(theta) / nrow(DTM)  # average probability over all paragraphs
names(topicProportions) <- topicNames     # Topic Names
sort(topicProportions, decreasing = TRUE) # sort
```

```{r}
topicProportions_df <- tibble(proportions = topicProportions, topic_names = names(topicProportions))
ggplot(topicProportions_df) +
  geom_col(mapping = aes(x=proportions , y=topic_names), width=.4) +
  theme(axis.text.y = element_text(size=16, 
    color="blue", 
    face="bold",
    angle=0))
```

```{r}
topicProportions_df
```

