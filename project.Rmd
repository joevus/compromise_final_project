---
title: "compromise words project"
author: "hoskisson"
date: "2022-12-04"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Ran `auth_setup_default()` and used browser to give rtweet authentication access.
Then whenever I want to use that same authentication, I run the line `auth_as("default")` in another R file.

```{r}
library(rtweet)
library(quanteda)
library(topicmodels)
library(tidyverse)
```

```{r}
auth_as("default")
```

```{r}
rstats <- search_tweets("#rstats", n = 2, include_rts = FALSE)
colnames(rstats)
rstats[1:5, c("created_at", "text", "id_str")]
```

```{r}
# another function could try:
# lookup_tweets
# collins_timeline <- get_timeline(user="SenatorCollins")

# corpus_collins <- corpus(collins_timeline, 
#                                     docid_field = "id",
#                                     text_field = "full_text")

# id from sen kevin cramer tweet march 19: 1505248314232651776
# tweet url: https://twitter.com/SenKevinCramer/status/1505248314232651776
# max id from kevin cramer tweet march 21: 1505904432088862732
# url: https://twitter.com/SenKevinCramer/status/1505904432088862732

tweets_from_list <- lists_statuses(
  list_id = 63915645,
  n = 100,
  max_id = tailed_tweets
)

tailed_tweets <- tweets_from_list %>%
  select(c(created_at, id, id_str, full_text)) %>%
  tail()
tailed_tweets
```


```{r}
# go backward on a senator's timeline to retrive a Tweet in the right timeframe
# the Tweet ids are used later for retrieving Tweets from the US Senator list

tweets_from_timeline <- get_timeline(
  user="PattyMurray",
  n = 20,
  max_id = tailed_tweets
)

tailed_tweets <- tweets_from_timeline %>%
  select(c(created_at, id, id_str, full_text)) %>%
  tail()

tailed_tweets

# View(tweets_from_timeline)
```

Get senator screen names
```{r}
members <- lists_members(list_id = 63915645 )
senator_handles_list <- members$screen_name
```

Respect for marriage act 2022
```{r}
tweets_from_timeline <- get_timeline(
  user=senator_handles_list,
  max_id = c("1598011580016304128"),
  since_id = c("1592524893316579329")
)
# Take only some fields. Get rid of fields that are lists because they cause an error
tweets_from_timeline_processed<-select(tweets_from_timeline, created_at, id, id_str, full_text, text)
write.csv(tweets_from_timeline_processed, "respect_for_marriage.csv" ,row.names = FALSE)
```

Inflation Reduction Act 2022
```{r}
tweets_from_timeline2 <- get_timeline(
  user=senator_handles_list,
  since_id = c("1551208949382258690"),
  max_id = c("1556772322832310272")
)

tweets_from_timeline2_processed<-select(tweets_from_timeline2, created_at, id, id_str, full_text, text)
write.csv(tweets_from_timeline2_processed, "inflation_reduction.csv" ,row.names = FALSE)
```

Bipartisan Safer Communities Act of 2022

```{r}
safer_communities_tweets <- get_timeline(
  user=senator_handles_list,
  since_id = c("1534892587399643137"),
  max_id = c("1540421395804463107")
)
safer_communities_tweets_processed<-select(safer_communities_tweets, created_at, id, id_str, full_text, text)
write.csv(safer_communities_tweets_processed, "safer_communities.csv" ,row.names = FALSE)
```

American Rescue Plan Act of 2021
```{r}
american_rescue_2021 <- get_timeline(
  user=senator_handles_list,
  since_id = c("1495486916082425857"),
  max_id = c("1500957088738160643")
)

american_rescue_2021_processed<-select(american_rescue_2021, created_at, id, id_str, full_text, text)
write.csv(american_rescue_2021_processed, "american_rescue_2021.csv", row.names = FALSE)
```


Infrastructure Investment and Jobs Act of 2021
```{r}
infrastructure_investment <- get_timeline(
  user=senator_handles_list,
  since_id = c("1420021866262040577"),
  max_id = c("1425616774146543618")
)

infrastructure_investment_processed<-select(infrastructure_investment, created_at, id, id_str, full_text, text)
write.csv(infrastructure_investment_processed, "infrastructure_investment.csv", row.names = FALSE)
```

## Lemma data

This is used for each topic model
```{r}
lemmaData <- read.csv2("baseform_en.tsv", # downloaded from https://github.com/tm4ss/tm4ss.github.io/tree/master/resources
                       sep=",", 
                       header=FALSE, 
                       encoding = "UTF-8", 
                       stringsAsFactors = F)
```



## Topic modeling for Respect for Marriage
```{r}
corpus_respect_for_marriage <- corpus(tweets_from_timeline,
                                      docid_field = "id",
                                      text_field= "full_text")

corpus_respect_for_marriage_proc <- tokens(corpus_respect_for_marriage,
                                        remove_punct = TRUE, # remove punctuation
                                        remove_numbers = TRUE, # remove numbers
                                        remove_symbols = TRUE) %>% # remove symbols (for social media data, could remove everything except letters)
                                          tokens_tolower() # remove capitalization



```

```{r}
corpus_respect_for_marriage_proc <-  tokens_replace(corpus_respect_for_marriage_proc, # "Substitute token types based on vectorized one-to-one matching"
                                    lemmaData$V1, 
                                    lemmaData$V2,
                                    valuetype = "fixed") 
```

```{r}
corpus_respect_for_marriage_proc <- corpus_respect_for_marriage_proc %>%
                             tokens_remove(stopwords("english")) %>%
                             tokens_ngrams(1) 
```


```{r}
#  Create dtm
DTM <- dfm(corpus_respect_for_marriage_proc)

# Minimum
minimumFrequency <- 10
DTM <- dfm_trim(DTM, 
                min_docfreq = minimumFrequency,
                max_docfreq = 99999999)

# keep only letters... brute force
DTM  <- dfm_select(DTM, 
                   pattern = "[a-z]", 
                   valuetype = "regex", 
                   selection = 'keep')
colnames(DTM) <- stringi::stri_replace_all_regex(colnames(DTM), 
                                                 "[^_a-z]","")

DTM <- dfm_compress(DTM, "features")

# We have several rows which do not have any content left. Drop them.

sel_idx <- rowSums(DTM) > 0
DTM <- DTM[sel_idx, ]
# textdata <- textdata[sel_idx, ]
```

```{r}
K <- 12
# Set seed to make results reproducible
set.seed(9161)
topicModel <- LDA(DTM, 
                  K, 
                  method="Gibbs", 
                  control=list(iter = 500, 
                               verbose = 25))
```

```{r}
tmResult <- posterior(topicModel)


# Topics are distributions over the entire vocabulary

beta <- tmResult$terms
glimpse(beta)


# Each doc has a distribution over k topics

theta <- tmResult$topics
glimpse(theta)               

terms(topicModel, 10)

# Top terms per topic. Use top 5 to interpret topics
top5termsPerTopic <- terms(topicModel, 
                           5)
# For the next steps, we want to give the topics more descriptive names than just numbers. Therefore, we simply concatenate the five most likely terms of each topic to a string that represents a pseudo-name for each topic.
topicNames <- apply(top5termsPerTopic, 
                    2, 
                    paste, 
                    collapse=" ")
```


```{r}
# source: http://jjacobs.me/tad/04_Topic_Modeling_ggplot2.html
respect_marriage_topics <- tidy(topicModel, matrix = "beta")

terms_per_topic <- 5
respect_marriage_top_terms <- respect_marriage_topics %>%
  group_by(topic) %>%
  top_n(terms_per_topic) %>%
  ungroup() %>%
  arrange(topic, -beta)

respect_marriage_top_terms$topic <- factor(respect_marriage_top_terms$topic)

respect_marriage_top_terms %>%
  mutate(term = reorder(term, beta)) %>%
  ggplot(aes(term, beta)) +
  geom_bar(stat = "identity") +
  facet_wrap(~ topic, scales = "free") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  coord_flip()
```


What are the most common themes?
```{r}
topicProportions <- colSums(theta) / nrow(DTM)  # average probability over all paragraphs
names(topicProportions) <- topicNames     # Topic Names
sort(topicProportions, decreasing = TRUE) # sort
```

```{r}
topicProportions_df <- tibble(proportions = topicProportions, topic_names = names(topicProportions))
ggplot(topicProportions_df) +
  geom_col(mapping = aes(x=proportions , y=topic_names), width=.4) +
  theme(axis.text.y = element_text(size=16, 
    color="blue", 
    face="bold",
    angle=0))
```

```{r}
topicProportions_df
```

## Topic Modeling



